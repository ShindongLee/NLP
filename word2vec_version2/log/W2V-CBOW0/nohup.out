loading...
preprocessing...
build training set...
Vocabulary size
71291

# of training samples
16718844

Loss : 7.310014
Loss : 7.123043
Loss : 7.111099
Loss : 7.064094
Loss : 7.003477
Loss : 6.996500
Loss : 7.131620
Loss : 7.049909
Loss : 7.064398
Loss : 7.059475
Loss : 6.713994
Loss : 7.018694
Loss : 6.857210
Loss : 7.016415
Loss : 6.788331
Loss : 6.961196
Loss : 7.071080
Loss : 6.795257
Loss : 6.549638
Loss : 6.822061
Loss : 6.541894
Loss : 6.722132
Loss : 6.767818
Loss : 6.809696
Loss : 6.985944
Loss : 6.715837
Loss : 6.890340
Loss : 6.644852
Loss : 6.811075
Loss : 6.715493
Loss : 7.017510
Loss : 6.829069
Loss : 6.769194
Loss : 6.892846
Loss : 6.841723
Loss : 6.828276
Loss : 6.619975
Loss : 6.956576
Loss : 6.983770
Loss : 6.906364
Loss : 6.921742
Loss : 6.676131
Loss : 6.812861
Loss : 6.909416
Loss : 6.827703
Loss : 6.896121
Loss : 6.899352
Loss : 6.463702
Loss : 6.462811
Loss : 6.877401
Loss : 6.743981
Loss : 6.801302
Loss : 6.643174
Loss : 6.720089
Loss : 6.696933
Loss : 6.726837
Loss : 6.832414
Loss : 6.679712
Loss : 6.856565
Loss : 6.791835
Loss : 6.657210
Loss : 6.685097
Loss : 6.590466
Loss : 6.754239
Loss : 6.752972
Loss : 6.550260
Loss : 6.859674
Loss : 6.723425
Loss : 6.844850
Loss : 6.726434
Loss : 6.889595
Loss : 6.701042
Loss : 6.857087
Loss : 6.657357
Loss : 6.679026
Loss : 6.709595
Loss : 6.733066
Loss : 6.779040
Loss : 6.730904
Loss : 6.671328
Loss : 6.502441
Loss : 6.434082
Loss : 6.341519
Loss : 6.692811
Loss : 6.908684
Loss : 6.843051
Loss : 6.651871
Loss : 6.666182
Loss : 6.639947
Loss : 6.864472
Loss : 6.432895
Loss : 6.848440
Loss : 6.771479
Loss : 6.629967
Loss : 6.603743
Loss : 6.717236
Loss : 6.762773
Loss : 6.612385
Loss : 6.651191
Loss : 6.721413
Loss : 6.543886
Loss : 6.700391
Loss : 6.965610
Loss : 6.650907
Loss : 6.729674
Loss : 6.858963
Loss : 6.683617
Loss : 6.790926
Loss : 6.814835
Loss : 6.756490
Loss : 6.738357
Loss : 6.753067
Loss : 6.690990
Loss : 6.707131
Loss : 6.523240
Loss : 6.650333
Loss : 6.726986
Loss : 6.728487
Loss : 6.732271
Loss : 6.796798
Loss : 6.689540
Loss : 6.553545
Loss : 6.379450
Loss : 6.658931
Loss : 6.620507
Loss : 6.682226
Loss : 6.606602
Loss : 6.289897
Loss : 6.607772
Loss : 6.652066
Loss : 6.634139
Loss : 6.757309
Loss : 6.672606
Loss : 6.559651
Loss : 6.568501
Loss : 6.637374
Loss : 6.711232
Loss : 6.476112
Loss : 6.742888
Loss : 6.885324
Loss : 6.699132
Loss : 6.492297
Loss : 6.549659
Loss : 6.594885
Loss : 6.255081
Loss : 6.353474
Loss : 6.571333
Loss : 6.726902
Loss : 6.520957
Loss : 6.606604
Loss : 6.448681
Loss : 6.605440
Loss : 6.715667
Loss : 6.546388
Loss : 6.645310
Loss : 6.698020
Loss : 6.549571
Loss : 6.510838
Loss : 6.623242
Loss : 6.602789
Loss : 6.697643
Loss : 6.505117
Loss : 6.572262
Loss : 6.427895
Loss : 6.514491
Loss : 6.541322
Loss : 6.592516
Loss : 6.575636
Loss : 6.597728
Loss : 6.503397
Loss : 6.159841
Loss : 6.617018
Loss : 6.566104
Loss : 6.403970
Loss : 6.486202
Loss : 6.740275
Loss : 7.004486
Loss : 6.317041
Loss : 6.577174
Loss : 6.642323
Loss : 6.624506
Loss : 6.763838
Loss : 6.449386
Loss : 6.573206
Loss : 6.544108
Loss : 6.460262
Loss : 6.536652
Loss : 6.588526
Loss : 6.241707
Loss : 6.031409
Loss : 6.613904
Loss : 6.488180
Loss : 6.602720
Loss : 6.482609
Loss : 6.616737
Loss : 6.536176
Loss : 6.639143
Loss : 6.598566
Loss : 6.712250
Loss : 6.597851
Loss : 6.534117
Loss : 6.658541
Loss : 6.642723
Loss : 6.672355
Loss : 6.465572
Loss : 6.496266
Loss : 6.399144
Loss : 6.216661
Loss : 6.458198
Loss : 6.544710
Loss : 6.629972
Loss : 6.574318
Loss : 6.545977
Loss : 6.550650
Loss : 6.376411
Loss : 6.447643
Loss : 6.405132
Loss : 6.393493
Loss : 6.509343
Loss : 6.599685
Loss : 6.449382
Loss : 6.593982
Loss : 6.587749
Loss : 6.463375
Loss : 6.461278
Loss : 6.577392
Loss : 6.634313
Loss : 6.635543
Loss : 6.521306
Loss : 6.536719
Loss : 6.477354
Loss : 6.406594
Loss : 6.057895
Loss : 6.510926
Loss : 6.410217
Loss : 6.510088
Loss : 6.540594
Loss : 6.644321
Loss : 6.457229
Loss : 5.975259
Loss : 6.515495
Loss : 6.577570
Loss : 6.532220
Loss : 6.456228
Loss : 6.603748
Loss : 6.362168
Loss : 6.337057
Loss : 6.618998
Loss : 6.340819
Loss : 6.191693
Loss : 5.780486
Loss : 5.646377
Loss : 5.629627
Loss : 6.108749
Loss : 6.274828
Loss : 6.132057
Loss : 5.975107
Loss : 6.217969
Loss : 6.212296
Loss : 6.427641
Loss : 6.242585
Loss : 6.496837
Loss : 6.438001
Loss : 6.450401
Loss : 6.397263
Loss : 6.455249
Loss : 6.602067
Loss : 6.132841
Loss : 6.414233
Loss : 6.442620
Loss : 6.568495
Loss : 6.583824
Loss : 6.589336
Loss : 6.620732
Loss : 6.619189
Loss : 6.471057
Loss : 6.550873
Loss : 6.485044
Loss : 6.582266
Loss : 6.469551
Loss : 6.484625
Loss : 6.095372
Loss : 5.885078
Loss : 6.449451
Loss : 6.381389
Loss : 6.376417
Loss : 6.556043
Loss : 6.510900
Loss : 6.438993
Loss : 6.516206
Loss : 6.348979
Loss : 6.532278
Loss : 6.500665
Loss : 6.332516
Loss : 6.629603
Loss : 6.601856
Loss : 6.658116
Loss : 6.567188
Loss : 6.158461
Loss : 6.647097
Loss : 6.636675
Loss : 6.483290
Loss : 6.130200
Loss : 5.981082
Loss : 6.081122
Loss : 5.765534
Loss : 6.363275
Loss : 6.300800
Loss : 6.389149
Loss : 6.344101
Loss : 6.541580
Loss : 6.418026
Loss : 6.010584
Loss : 6.109204
Loss : 6.585514
Loss : 6.228868
Loss : 6.411742
Loss : 6.653905
Loss : 6.572943
Loss : 6.402318
Loss : 6.492377
Loss : 6.331570
Loss : 6.328221
Loss : 6.362052
Loss : 6.152815
Loss : 6.432031
Loss : 6.188710
Loss : 6.613775
Loss : 6.297614
Loss : 6.544864
Loss : 6.469848
Loss : 6.598921
Loss : 6.716043
Loss : 6.574087
Time Elapsed for Training 01:02:08
word2vec.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  embedding = torch.tensor(embedding)
word2vec.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_vector = torch.tensor(x_vector)
Questions = 17827, Correct Questions = 1548, Hitting Rate = 8.6835
Time Elapsed for Validaiton 02:58:40
