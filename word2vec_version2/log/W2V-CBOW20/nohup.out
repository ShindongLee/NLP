loading...
preprocessing...
build training set...
Vocabulary size
71291

# of training samples
16718844

Loss : 8.626263
Loss : 4.958642
Loss : 4.439106
Loss : 4.137255
Loss : 4.110571
Loss : 3.851200
Loss : 3.842073
Loss : 3.869848
Loss : 3.768646
Loss : 3.730266
Loss : 3.563856
Loss : 3.693969
Loss : 3.639643
Loss : 3.660170
Loss : 3.530992
Loss : 3.561625
Loss : 3.646918
Loss : 3.480284
Loss : 3.360663
Loss : 3.462909
Loss : 3.332793
Loss : 3.398729
Loss : 3.460603
Loss : 3.474193
Loss : 3.552203
Loss : 3.389852
Loss : 3.487167
Loss : 3.365545
Loss : 3.423536
Loss : 3.382656
Loss : 3.489202
Loss : 3.389432
Loss : 3.378729
Loss : 3.411108
Loss : 3.429308
Loss : 3.351441
Loss : 3.302179
Loss : 3.397169
Loss : 3.469379
Loss : 3.389917
Loss : 3.446441
Loss : 3.315841
Loss : 3.324594
Loss : 3.432281
Loss : 3.362832
Loss : 3.377247
Loss : 3.357450
Loss : 3.153372
Loss : 3.115495
Loss : 3.357031
Loss : 3.286410
Loss : 3.269065
Loss : 3.174404
Loss : 3.244364
Loss : 3.262458
Loss : 3.211132
Loss : 3.318789
Loss : 3.220315
Loss : 3.309278
Loss : 3.288567
Loss : 3.216815
Loss : 3.182634
Loss : 3.164312
Loss : 3.174823
Loss : 3.231570
Loss : 3.187184
Loss : 3.263379
Loss : 3.247960
Loss : 3.258156
Loss : 3.205304
Loss : 3.315314
Loss : 3.239474
Loss : 3.320165
Loss : 3.210918
Loss : 3.220727
Loss : 3.219334
Loss : 3.210832
Loss : 3.210606
Loss : 3.180196
Loss : 3.152476
Loss : 3.053844
Loss : 3.011770
Loss : 2.994410
Loss : 3.224764
Loss : 3.256231
Loss : 3.234124
Loss : 3.125310
Loss : 3.148821
Loss : 3.154261
Loss : 3.213767
Loss : 3.075572
Loss : 3.191601
Loss : 3.148152
Loss : 3.127400
Loss : 3.102050
Loss : 3.173866
Loss : 3.200631
Loss : 3.048858
Loss : 2.966313
Loss : 3.178266
Loss : 3.024964
Loss : 3.124193
Loss : 3.230250
Loss : 3.098166
Loss : 3.167588
Loss : 3.211926
Loss : 3.180155
Loss : 3.195481
Loss : 3.164457
Loss : 3.158015
Loss : 3.202186
Loss : 3.190631
Loss : 3.117957
Loss : 3.130222
Loss : 3.058523
Loss : 3.088562
Loss : 3.154525
Loss : 3.089919
Loss : 3.165545
Loss : 3.143025
Loss : 3.140984
Loss : 3.058098
Loss : 2.964119
Loss : 3.104485
Loss : 3.140664
Loss : 3.124524
Loss : 3.053468
Loss : 2.914012
Loss : 3.050777
Loss : 3.082202
Loss : 3.091810
Loss : 3.119292
Loss : 3.100275
Loss : 3.050509
Loss : 3.082729
Loss : 3.111132
Loss : 3.094608
Loss : 3.015537
Loss : 3.130301
Loss : 3.188760
Loss : 3.108062
Loss : 3.013506
Loss : 2.987892
Loss : 3.086614
Loss : 2.884138
Loss : 2.940370
Loss : 3.063940
Loss : 3.105747
Loss : 2.964585
Loss : 3.051039
Loss : 2.972121
Loss : 3.043689
Loss : 3.114393
Loss : 3.026876
Loss : 3.059698
Loss : 3.092329
Loss : 3.005536
Loss : 3.024974
Loss : 3.041550
Loss : 3.029345
Loss : 3.057238
Loss : 2.950951
Loss : 3.024757
Loss : 2.925807
Loss : 2.968396
Loss : 2.981157
Loss : 3.006292
Loss : 3.034782
Loss : 2.990889
Loss : 2.980360
Loss : 2.837525
Loss : 3.033753
Loss : 2.993923
Loss : 2.953951
Loss : 2.976774
Loss : 3.079886
Loss : 2.953737
Loss : 2.882453
Loss : 2.999024
Loss : 3.067379
Loss : 2.992395
Loss : 3.104365
Loss : 2.966779
Loss : 3.024581
Loss : 3.012180
Loss : 2.946726
Loss : 3.016592
Loss : 2.989305
Loss : 2.866181
Loss : 2.697956
Loss : 3.073004
Loss : 3.034891
Loss : 3.001261
Loss : 2.897701
Loss : 3.032336
Loss : 3.018431
Loss : 3.023590
Loss : 3.025825
Loss : 3.063089
Loss : 3.013997
Loss : 3.001952
Loss : 3.009474
Loss : 3.026278
Loss : 3.048993
Loss : 2.977576
Loss : 2.965699
Loss : 2.935029
Loss : 2.824193
Loss : 2.976872
Loss : 2.957656
Loss : 3.015382
Loss : 2.991125
Loss : 2.999831
Loss : 2.944354
Loss : 2.949918
Loss : 2.968796
Loss : 2.913152
Loss : 2.875271
Loss : 2.929328
Loss : 3.003199
Loss : 2.960607
Loss : 3.027688
Loss : 2.986216
Loss : 2.965510
Loss : 2.926599
Loss : 2.982027
Loss : 3.041422
Loss : 2.962979
Loss : 2.948323
Loss : 2.968577
Loss : 2.940549
Loss : 2.909081
Loss : 2.702963
Loss : 2.920792
Loss : 2.903844
Loss : 2.943549
Loss : 2.923548
Loss : 2.987478
Loss : 2.918581
Loss : 2.752205
Loss : 2.936551
Loss : 2.955365
Loss : 2.934549
Loss : 2.883960
Loss : 2.975740
Loss : 2.844848
Loss : 2.889056
Loss : 2.988434
Loss : 2.897010
Loss : 2.806583
Loss : 2.630791
Loss : 2.554817
Loss : 2.532653
Loss : 2.706446
Loss : 2.829354
Loss : 2.782688
Loss : 2.694127
Loss : 2.821505
Loss : 2.804137
Loss : 2.914677
Loss : 2.833331
Loss : 2.927606
Loss : 2.913629
Loss : 2.927411
Loss : 2.914450
Loss : 2.904955
Loss : 2.976026
Loss : 2.724151
Loss : 2.905936
Loss : 2.915987
Loss : 2.981245
Loss : 2.940203
Loss : 2.978433
Loss : 2.986201
Loss : 2.977713
Loss : 2.954067
Loss : 2.948843
Loss : 2.921134
Loss : 2.935711
Loss : 2.902285
Loss : 2.894288
Loss : 2.680695
Loss : 2.580668
Loss : 2.938671
Loss : 2.886434
Loss : 2.821343
Loss : 2.916594
Loss : 2.944361
Loss : 2.872862
Loss : 2.866925
Loss : 2.854041
Loss : 2.930127
Loss : 2.940492
Loss : 2.796542
Loss : 2.938358
Loss : 2.943093
Loss : 3.003699
Loss : 2.914464
Loss : 2.735540
Loss : 2.946227
Loss : 2.945533
Loss : 2.888726
Loss : 2.720312
Loss : 2.622213
Loss : 2.703575
Loss : 2.484038
Loss : 2.846565
Loss : 2.822520
Loss : 2.861834
Loss : 2.784173
Loss : 2.942581
Loss : 2.885846
Loss : 2.692739
Loss : 2.705539
Loss : 2.899917
Loss : 2.814059
Loss : 2.876075
Loss : 2.977966
Loss : 2.916309
Loss : 2.832585
Loss : 2.938322
Loss : 2.848282
Loss : 2.828062
Loss : 2.836362
Loss : 2.795264
Loss : 2.865267
Loss : 2.787059
Loss : 2.955425
Loss : 2.823586
Loss : 2.925106
Loss : 2.897087
Loss : 2.958424
Loss : 2.983158
Loss : 2.914300
Time Elapsed for Training 00:47:40
word2vec.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  embedding = torch.tensor(embedding)
word2vec.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_vector = torch.tensor(x_vector)
Questions = 17827, Correct Questions = 994, Hitting Rate = 5.5758
Time Elapsed for Validaiton 02:57:05
