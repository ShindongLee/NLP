loading...
preprocessing...
build training set...
Vocabulary size
71291

# of training samples
16718844

Time Elapsed for SubSampling 00:00:06

Training Set before Subsampling : 16718844
Number of discard : 5015642
Training Set after Subsampling : 11703202
Loss : 8.497776
Loss : 5.163369
Loss : 4.626432
Loss : 4.410989
Loss : 4.078900
Loss : 4.177988
Loss : 4.017292
Loss : 3.957865
Loss : 3.975514
Loss : 3.948113
Loss : 3.804666
Loss : 3.908641
Loss : 3.738465
Loss : 3.768816
Loss : 3.652873
Loss : 3.725566
Loss : 3.824377
Loss : 3.723790
Loss : 3.705677
Loss : 3.719423
Loss : 3.717510
Loss : 3.687472
Loss : 3.711682
Loss : 3.711232
Loss : 3.610850
Loss : 3.655680
Loss : 3.709817
Loss : 3.656332
Loss : 3.656587
Loss : 3.595006
Loss : 3.676238
Loss : 3.611390
Loss : 3.555340
Loss : 3.365473
Loss : 3.610371
Loss : 3.488550
Loss : 3.445076
Loss : 3.487978
Loss : 3.488149
Loss : 3.541743
Loss : 3.492049
Loss : 3.548492
Loss : 3.393105
Loss : 3.365106
Loss : 3.508621
Loss : 3.511296
Loss : 3.542608
Loss : 3.463135
Loss : 3.456208
Loss : 3.535414
Loss : 3.555706
Loss : 3.489179
Loss : 3.449159
Loss : 3.397665
Loss : 3.392317
Loss : 3.373398
Loss : 3.253709
Loss : 3.216521
Loss : 3.545347
Loss : 3.418045
Loss : 3.358135
Loss : 3.368102
Loss : 3.392270
Loss : 3.369968
Loss : 3.360394
Loss : 3.308426
Loss : 3.358814
Loss : 3.458178
Loss : 3.290342
Loss : 3.146233
Loss : 3.317966
Loss : 3.330015
Loss : 3.375120
Loss : 3.415811
Loss : 3.426727
Loss : 3.383733
Loss : 3.379309
Loss : 3.405177
Loss : 3.366150
Loss : 3.335582
Loss : 3.272294
Loss : 3.339241
Loss : 3.317682
Loss : 3.391114
Loss : 3.343512
Loss : 3.197062
Loss : 3.358473
Loss : 3.372189
Loss : 3.269868
Loss : 3.203269
Loss : 3.298481
Loss : 3.309227
Loss : 3.305691
Loss : 3.318961
Loss : 3.337861
Loss : 3.309806
Loss : 3.260246
Loss : 3.386468
Loss : 3.326915
Loss : 3.190130
Loss : 3.247657
Loss : 3.064788
Loss : 3.272021
Loss : 3.249553
Loss : 3.182731
Loss : 3.240320
Loss : 3.287086
Loss : 3.246154
Loss : 3.276794
Loss : 3.249851
Loss : 3.197709
Loss : 3.279257
Loss : 3.259103
Loss : 3.174302
Loss : 3.145996
Loss : 3.142710
Loss : 3.207469
Loss : 3.261810
Loss : 3.162947
Loss : 3.098709
Loss : 3.246432
Loss : 3.176451
Loss : 3.213848
Loss : 3.245969
Loss : 3.065677
Loss : 3.188619
Loss : 3.216713
Loss : 3.273805
Loss : 3.217421
Loss : 3.173121
Loss : 3.156103
Loss : 3.168994
Loss : 3.010546
Loss : 3.121591
Loss : 3.221582
Loss : 3.129525
Loss : 3.219934
Loss : 3.214310
Loss : 3.229828
Loss : 3.211566
Loss : 3.228770
Loss : 3.217086
Loss : 3.221381
Loss : 3.192612
Loss : 3.128191
Loss : 3.058132
Loss : 3.185391
Loss : 3.166462
Loss : 3.192789
Loss : 3.158777
Loss : 3.174477
Loss : 3.145361
Loss : 3.066494
Loss : 3.142144
Loss : 3.171243
Loss : 3.234834
Loss : 3.154639
Loss : 3.157063
Loss : 3.195877
Loss : 3.152334
Loss : 3.144843
Loss : 3.111309
Loss : 3.075428
Loss : 2.958837
Loss : 3.070684
Loss : 3.073243
Loss : 3.140420
Loss : 3.086718
Loss : 3.046602
Loss : 3.106419
Loss : 3.082135
Loss : 3.117268
Loss : 3.046304
Loss : 3.112536
Loss : 3.069593
Loss : 2.970379
Loss : 2.809944
Loss : 2.847059
Loss : 3.037792
Loss : 2.944847
Loss : 3.009530
Loss : 3.099295
Loss : 3.052320
Loss : 3.114932
Loss : 3.106039
Loss : 3.125426
Loss : 3.121336
Loss : 2.955731
Loss : 3.078721
Loss : 3.175696
Loss : 3.142529
Loss : 3.180574
Loss : 3.178993
Loss : 3.130227
Loss : 3.071080
Loss : 3.071811
Loss : 3.006027
Loss : 2.742957
Loss : 3.021545
Loss : 3.043803
Loss : 3.051443
Loss : 3.066539
Loss : 3.000056
Loss : 3.144672
Loss : 3.099047
Loss : 3.000066
Loss : 3.097661
Loss : 3.181856
Loss : 3.055923
Loss : 2.958163
Loss : 3.089962
Loss : 3.014735
Loss : 2.752760
Loss : 2.770300
Loss : 2.867879
Loss : 3.053609
Loss : 2.968194
Loss : 3.086462
Loss : 2.977409
Loss : 2.900845
Loss : 3.007790
Loss : 3.047120
Loss : 3.137596
Loss : 2.979729
Loss : 3.095354
Loss : 2.995613
Loss : 2.989135
Loss : 3.084286
Loss : 2.988990
Loss : 3.094868
Loss : 3.075902
Loss : 3.062281
Loss : 3.137100
Loss : 3.080656
Time Elapsed for Training 00:34:04
word2vec.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  embedding = torch.tensor(embedding)
word2vec.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_vector = torch.tensor(x_vector)
Questions = 17827, Correct Questions = 959, Hitting Rate = 5.3795
Time Elapsed for Validaiton 03:05:10
