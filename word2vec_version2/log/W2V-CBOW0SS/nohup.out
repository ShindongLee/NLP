loading...
preprocessing...
build training set...
Vocabulary size
71291

# of training samples
16718844

Time Elapsed for SubSampling 00:00:06

Training Set before Subsampling : 16718844
Number of discard : 5016123
Training Set after Subsampling : 11702721
Loss : 8.521923
Loss : 8.367735
Loss : 8.369580
Loss : 8.400086
Loss : 8.330024
Loss : 8.425651
Loss : 8.343709
Loss : 8.264782
Loss : 8.297484
Loss : 8.331352
Loss : 8.341027
Loss : 8.345143
Loss : 8.116009
Loss : 8.256811
Loss : 8.113540
Loss : 8.151940
Loss : 8.286387
Loss : 8.164270
Loss : 8.109928
Loss : 8.224421
Loss : 8.189602
Loss : 8.234846
Loss : 8.242129
Loss : 8.289230
Loss : 8.218046
Loss : 8.189932
Loss : 8.278813
Loss : 8.178758
Loss : 8.217511
Loss : 8.107452
Loss : 8.240055
Loss : 8.132731
Loss : 8.127430
Loss : 7.763549
Loss : 8.208195
Loss : 8.092629
Loss : 7.957244
Loss : 8.027832
Loss : 8.113522
Loss : 8.121526
Loss : 8.031958
Loss : 8.143404
Loss : 7.964663
Loss : 7.957898
Loss : 8.134855
Loss : 8.045236
Loss : 8.212147
Loss : 8.027619
Loss : 8.057898
Loss : 8.143565
Loss : 8.086269
Loss : 8.032338
Loss : 7.948855
Loss : 7.930843
Loss : 7.897071
Loss : 7.925680
Loss : 7.779866
Loss : 7.642907
Loss : 8.126738
Loss : 8.059557
Loss : 7.928273
Loss : 7.904007
Loss : 8.004177
Loss : 7.932679
Loss : 7.993355
Loss : 7.841656
Loss : 7.887818
Loss : 8.101481
Loss : 7.867940
Loss : 7.690963
Loss : 7.997636
Loss : 7.948130
Loss : 8.025333
Loss : 8.074806
Loss : 8.045951
Loss : 8.032916
Loss : 8.025188
Loss : 7.982856
Loss : 7.988307
Loss : 7.935157
Loss : 7.806088
Loss : 7.969132
Loss : 7.975229
Loss : 8.069520
Loss : 7.940583
Loss : 7.647729
Loss : 7.994671
Loss : 7.911017
Loss : 7.840800
Loss : 7.719977
Loss : 7.951469
Loss : 7.902129
Loss : 7.956231
Loss : 7.920320
Loss : 7.924480
Loss : 7.988637
Loss : 7.823159
Loss : 8.080589
Loss : 7.937278
Loss : 7.694139
Loss : 7.786383
Loss : 7.488628
Loss : 7.839832
Loss : 7.801378
Loss : 7.772483
Loss : 7.825634
Loss : 7.924798
Loss : 7.833273
Loss : 7.876156
Loss : 7.878615
Loss : 7.737624
Loss : 7.900050
Loss : 7.897250
Loss : 7.827138
Loss : 7.601635
Loss : 7.700339
Loss : 7.795722
Loss : 7.859877
Loss : 7.749701
Loss : 7.550640
Loss : 7.901640
Loss : 7.751892
Loss : 7.833442
Loss : 7.929378
Loss : 7.659667
Loss : 7.777353
Loss : 7.853347
Loss : 7.937729
Loss : 7.797616
Loss : 7.712178
Loss : 7.666029
Loss : 7.756935
Loss : 7.423270
Loss : 7.662680
Loss : 7.762265
Loss : 7.739046
Loss : 7.890662
Loss : 7.802874
Loss : 7.891219
Loss : 7.812940
Loss : 7.862218
Loss : 7.893624
Loss : 7.910300
Loss : 7.776191
Loss : 7.657973
Loss : 7.528860
Loss : 7.772609
Loss : 7.802128
Loss : 7.822337
Loss : 7.795534
Loss : 7.733831
Loss : 7.688823
Loss : 7.632016
Loss : 7.774391
Loss : 7.757727
Loss : 7.873997
Loss : 7.723950
Loss : 7.770515
Loss : 7.836430
Loss : 7.819590
Loss : 7.810237
Loss : 7.590333
Loss : 7.663793
Loss : 7.448292
Loss : 7.624440
Loss : 7.606609
Loss : 7.831066
Loss : 7.659728
Loss : 7.445533
Loss : 7.716226
Loss : 7.728665
Loss : 7.726941
Loss : 7.598236
Loss : 7.653012
Loss : 7.541802
Loss : 7.346283
Loss : 7.065338
Loss : 7.210188
Loss : 7.584865
Loss : 7.364802
Loss : 7.438146
Loss : 7.708953
Loss : 7.563817
Loss : 7.725512
Loss : 7.697516
Loss : 7.706976
Loss : 7.784756
Loss : 7.399874
Loss : 7.668291
Loss : 7.888930
Loss : 7.834465
Loss : 7.930542
Loss : 7.893953
Loss : 7.761861
Loss : 7.654572
Loss : 7.674435
Loss : 7.554334
Loss : 7.147498
Loss : 7.575442
Loss : 7.580385
Loss : 7.687190
Loss : 7.653741
Loss : 7.646667
Loss : 7.803010
Loss : 7.721482
Loss : 7.582598
Loss : 7.794502
Loss : 7.891381
Loss : 7.687254
Loss : 7.510453
Loss : 7.795941
Loss : 7.602696
Loss : 7.143477
Loss : 7.170149
Loss : 7.334761
Loss : 7.675078
Loss : 7.528160
Loss : 7.735755
Loss : 7.481001
Loss : 7.370748
Loss : 7.617280
Loss : 7.656332
Loss : 7.857742
Loss : 7.549276
Loss : 7.696819
Loss : 7.540866
Loss : 7.550990
Loss : 7.686658
Loss : 7.519780
Loss : 7.740485
Loss : 7.719067
Loss : 7.714346
Loss : 7.870277
Loss : 7.741500
Time Elapsed for Training 00:47:06
word2vec.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  embedding = torch.tensor(embedding)
word2vec.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_vector = torch.tensor(x_vector)
Questions = 17827, Correct Questions = 1474, Hitting Rate = 8.2684
Time Elapsed for Validaiton 02:56:43
